"use strict";(self.webpackChunkyoujiang_site=self.webpackChunkyoujiang_site||[]).push([[896],{2314:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var o=n(4848),i=n(8453);const r={},s=void 0,a={permalink:"/blog/yolo-live-demo",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/yolo-live-demo.md",source:"@site/blog/yolo-live-demo.md",title:"yolo-live-demo",description:"Hello everyone, my name\u2019s Youjiang, I\u2019m the Edge AI application engineer at Seeed Studio. It\u2019s my pleasure to be here with you to discover the potential of the Jetson Orin and YOLOv8, within the retail market. For this live demo, I'll show you how to use Ultralytics YOLOv8 to implement queue management and deploy the application on our reComputer Jetson Orin NX edge device.",date:"2024-04-10T16:41:19.000Z",formattedDate:"April 10, 2024",tags:[],readingTime:3.85,hasTruncateMarker:!1,authors:[],frontMatter:{},unlisted:!1,nextItem:{title:"Update Site",permalink:"/blog/person-info/update-site"}},c={authorsImageUrls:[]},l=[];function h(e){const t={code:"code",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:"Hello everyone, my name\u2019s Youjiang, I\u2019m the Edge AI application engineer at Seeed Studio. It\u2019s my pleasure to be here with you to discover the potential of the Jetson Orin and YOLOv8, within the retail market. For this live demo, I'll show you how to use Ultralytics YOLOv8 to implement queue management and deploy the application on our reComputer Jetson Orin NX edge device."}),"\n",(0,o.jsx)(t.p,{children:"So now, let's execute the python script to see what happens. All of the inferencing parts take place on the Jetson device."}),"\n",(0,o.jsx)(t.p,{children:"(\u64ad\u653edemo\u6700\u7ec8\u751f\u6210\u7684\u89c6\u9891)"}),"\n",(0,o.jsx)(t.p,{children:"In fact, I only spent four hours doing this. It took me three hours to find the input video, and I just spent one hour to setup Jetson runtime environment and modifying the test code. This means that both Jetson and Ultralytics are very user-friendly."}),"\n",(0,o.jsx)(t.p,{children:"Doing such a demo roughly requires three steps."}),"\n",(0,o.jsx)(t.p,{children:"1\u3001First of all, setting up the Yolov8 running environment on Jetson. It has become a very simple task. Users can refer to ultralytics documents or seeed documents to install Ultralytics very easily."}),"\n",(0,o.jsx)(t.p,{children:"2\u3001Next is writing the application. Here, we can also refer to the Ultralytics official documentation to design our own program. For ease of demonstration, we directly copy the example code from the documentation and modify the relevant parameters, such as the path of the input file, queuing region, etc."}),"\n",(0,o.jsx)(t.p,{children:"3\u3001Finally, run the script of this application using the Python interpreter."}),"\n",(0,o.jsx)(t.p,{children:"Let's check the python code."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sh",children:'# import some dependencies at the beginning.\n\nimport cv2\nfrom ultralytics import YOLO\nfrom ultralytics.solutions import queue_management\n\n\n# Load the YOLO model, here you can choose which model to use.\nmodel = YOLO("yolov8n.pt")\n\n# Select the path of the input video. It can also be an IP camera, USB camera, and so on.\ncap = cv2.VideoCapture("/home/seeed/queue.mp4")\n\n# Check if the input video stream starts normally. if abnormally , the program will automatically exit.\nassert cap.isOpened(), "Error reading video file"\n\n# Create a new video file to save the inference results.\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter("queue_management.avi", cv2.VideoWriter_fourcc(*\'mp4v\'), fps, (int(w*0.5), int(h*0.5)))\n\n# Define the queuing region. This is very interesting. \n# I have an input video, but I don\'t know how to determine the coordinates of each point. \n# Therefore, I use OpenCV to open a frame of the input video,\n# and then determine each point in the OpenCV window.\n\nLet me show you how to do it. \n(\u6dfb\u52a0\u8fd9\u884c\u4ee3\u7801\u5e76\u8fd0\u884c cv2.waitKey(0) \u5c4f\u5e55\u4e0a\u5c06\u4f1a\u51fa\u73b0\u4e00\u4e2a\u65b0\u7684\u7a97\u53e3\uff0c\u5728\u65b0\u7a97\u53e3\u4e2d\u79fb\u52a8\u9f20\u6807\u5c06\u4f1a\u5b9e\u65f6\u663e\u793a\u9f20\u6807\u7684\u5750\u6807)\nWhen I move the mouse in this window, the bottom right corner will display the coordinates of the mouse in the image in real-time. Then, I just need to record the points I need.\n\nqueue_region = [(380, 30), (940, 30), (940, 510), (380, 510)]\n\n# Initialize the queue management object.\nqueue = queue_management.QueueManager()\nqueue.set_args(classes_names=model.names, reg_pts=queue_region, line_thickness=3, fontsize=1.0, region_color=(255, 144, 31))\n\n# Retrieve and process each frame from the video file.\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if success:\n# resize the input frame\n        im0 = cv2.resize(im0, (0, 0), fx=0.5, fy=0.5)\n# Invoke the YOLOv8 model to track the person object.\n        tracks = model.track(im0, show=False, persist=True, verbose=False, classes=0)\n# Invoke the queue management method provided by Ultralytics.\n        out = queue.process_queue(im0, tracks)\n# save the result\n        video_writer.write(im0)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n        continue\n    print("Video frame is empty or video processing has been successfully completed.")\n    break\n#  close video and window\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,o.jsx)(t.p,{children:"However, the final output of these current application is edited video, which is intended for human checking, it can not provide direct information to machines or other applications. So, I've customized a new class that base on ultralytics to direct output people flow information. Let's run this script to try it out."}),"\n",(0,o.jsx)(t.p,{children:"(\u8fd0\u884c\u7b2c\u4e8c\u4e2a\u811a\u672c)"}),"\n",(0,o.jsx)(t.p,{children:"You can see that the number of object in the defined region is shown in real time in the terminal."}),"\n",(0,o.jsx)(t.p,{children:"In fact, ultralytics can do much more than that. We can also use it to generate density maps to help retail stores plan store space in a better way.\nSo I'm going to show you this demo of a heat map."}),"\n",(0,o.jsx)(t.p,{children:"\uff08\u8fd0\u884c\u7b2c\u4e09\u4e2a\u811a\u672c\uff09"}),"\n",(0,o.jsx)(t.p,{children:"We can clearly see the distribution density of customers in each region during a certain period of time."}),"\n",(0,o.jsx)(t.p,{children:"That is all of the live demo presentation. Thank you very much."})]})}function d(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>a});var o=n(6540);const i={},r=o.createContext(i);function s(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:t},e.children)}}}]);